# Quantisation_and_LoRA
量子化した状態のモデルを保存し、ファインチューニングしてみる。
私のパソコンは、一般に比べれば十分協力だが、AIで色々とやるには貧弱だ。大きなモデルを実用的に扱うためにはファインチューニングする必要があるが難しい。
悩んでいるところに下記の記事を見つけ、実際に作ってみようと考えた。
専門的な本を詠んだ事も無く講義も受けたことが無い私にうまく行くか分からないが、やるだけやってみようと思っている。

#参考にした記事
AutoGPTQ と transformers によるLLMの軽量化  https://note.com/npaka/n/nb4b1ef2f77cf
